Awesomeâ€”your setup is perfect for adding a website-grounded â€œanswer only from aihub.org.zaâ€ brain. Below is a complete, no-guesswork path you can drop into your Rasa project. It covers crawling, indexing, and a custom retrieval action that refuses to answer outside the siteâ€™s content.

---

# ðŸš€ Add website-grounded answers (aihub.org.za) to Rasa

## 0) Install extra deps (crawler + embeddings + vector index)

From your project root (inside `.venv`):

```bash
pip install beautifulsoup4 trafilatura readability-lxml markdownify
pip install sentence-transformers faiss-cpu numpy scikit-learn
```

> Why these?
>
> * `trafilatura`/`readability-lxml` = robust article extraction
> * `sentence-transformers` + `faiss-cpu` = fast local similarity search (no cloud)
> * `markdownify` = nicer text for display/citations

---

## 1) Crawl the site and extract clean text

Create `tools/crawl_site.py`:

```python
#!/usr/bin/env python3
import os, re, time, json, urllib.parse, hashlib, pathlib, sys
import requests
from bs4 import BeautifulSoup
import trafilatura

START_URL = "https://aihub.org.za/"
ALLOWED_HOST = urllib.parse.urlparse(START_URL).netloc
OUT_DIR = "kb/raw"
MAX_PAGES = 500
TIMEOUT = 12
HEADERS = {"User-Agent": "RasaCrawler/1.0 (+https://aihub.org.za/)"}

def normalize_url(url, base):
    u = urllib.parse.urljoin(base, url.split("#")[0])
    p = urllib.parse.urlparse(u)
    if p.netloc != ALLOWED_HOST:
        return None
    if p.scheme not in ("http", "https"):
        return None
    return urllib.parse.urlunparse((p.scheme, p.netloc, p.path.rstrip("/"), "", "", ""))

def fetch(url):
    try:
        r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
        if r.status_code == 200 and "text/html" in r.headers.get("Content-Type",""):
            return r.text
    except requests.RequestException:
        pass
    return None

def extract_clean(html, url):
    # prefer trafilatura; fallback to basic soup text
    txt = trafilatura.extract(html, url=url, output="text", include_links=False) or ""
    if not txt.strip():
        soup = BeautifulSoup(html, "html.parser")
        for s in soup(["script","style","noscript"]): s.decompose()
        txt = soup.get_text(" ", strip=True)
    # de-dup whitespace
    txt = re.sub(r"\s+\n", "\n", re.sub(r"[ \t]+", " ", txt)).strip()
    return txt

def save_doc(url, text):
    os.makedirs(OUT_DIR, exist_ok=True)
    hid = hashlib.sha1(url.encode()).hexdigest()[:16]
    with open(os.path.join(OUT_DIR, f"{hid}.json"), "w", encoding="utf-8") as f:
        json.dump({"url": url, "text": text}, f, ensure_ascii=False)

def main():
    to_visit = [START_URL]
    seen = set()
    pages = 0
    while to_visit and pages < MAX_PAGES:
        url = to_visit.pop(0)
        if url in seen: 
            continue
        seen.add(url)
        html = fetch(url)
        if not html: 
            continue
        text = extract_clean(html, url)
        if len(text) > 300:  # skip super-short pages
            save_doc(url, text)
            pages += 1
            print(f"[{pages}] {url} ({len(text)} chars)")
        soup = BeautifulSoup(html, "html.parser")
        for a in soup.find_all("a", href=True):
            nu = normalize_url(a["href"], url)
            if nu and nu not in seen:
                to_visit.append(nu)
        time.sleep(0.25)  # be polite
    print(f"Done. Saved {pages} pages to {OUT_DIR}")

if __name__ == "__main__":
    main()
```

Run:

```bash
python tools/crawl_site.py
```

This writes cleaned page JSON under `kb/raw/`.

---

## 2) Build a local vector index (FAISS)

Create `tools/index_kb.py`:

```python
#!/usr/bin/env python3
import os, json, glob, math
from typing import List, Dict
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss

RAW_DIR = "kb/raw"
IDX_DIR = "kb/index"
CHUNK_SIZE = 900        # characters
CHUNK_OVERLAP = 150     # characters

def chunk_text(text: str, url: str):
    chunks = []
    i = 0
    while i < len(text):
        chunk = text[i:i+CHUNK_SIZE]
        if chunk.strip():
            chunks.append({"url": url, "text": chunk.strip()})
        i += CHUNK_SIZE - CHUNK_OVERLAP
    return chunks

def load_docs() -> List[Dict]:
    docs = []
    for fp in glob.glob(os.path.join(RAW_DIR, "*.json")):
        with open(fp, "r", encoding="utf-8") as f:
            j = json.load(f)
            docs.extend(chunk_text(j["text"], j["url"]))
    return docs

def main():
    os.makedirs(IDX_DIR, exist_ok=True)
    print("Loading docsâ€¦")
    docs = load_docs()
    if not docs:
        raise SystemExit("No crawled docs found. Run the crawler first.")
    texts = [d["text"] for d in docs]
    print(f"Chunks: {len(texts)}")

    print("Embeddingâ€¦")
    model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
    X = model.encode(texts, convert_to_numpy=True, normalize_embeddings=True).astype("float32")

    print("Indexingâ€¦")
    index = faiss.IndexFlatIP(X.shape[1])  # cosine if normalized
    index.add(X)

    np.save(os.path.join(IDX_DIR, "embeddings.npy"), X)
    with open(os.path.join(IDX_DIR, "meta.json"), "w", encoding="utf-8") as f:
        json.dump(docs, f, ensure_ascii=False)
    faiss.write_index(index, os.path.join(IDX_DIR, "faiss.index"))
    print("OK. Saved to kb/index/")

if __name__ == "__main__":
    main()
```

Run:

```bash
python tools/index_kb.py
```

Result: `kb/index/faiss.index`, `embeddings.npy`, `meta.json`.

---

## 3) Wire Rasa to the index via a custom action

Update `domain.yml`:

```yaml
intents:
  - ask_kb
  - out_of_scope

responses:
  utter_out_of_scope:
    - text: "I can help with information from aihub.org.za. Could you rephrase your question about AiHub?"

actions:
  - action_retrieve_answer
```

Add a simple routing rule (create/update `rules.yml`):

```yaml
version: "3.1"
rules:
  - rule: Answer knowledge questions via retriever
    steps:
      - intent: ask_kb
      - action: action_retrieve_answer

  - rule: Default out-of-scope
    steps:
      - intent: out_of_scope
      - action: utter_out_of_scope
```

Tweak `config.yml` (fallbacks + basic NLU; keep your existing settings and add a stricter fallback):

```yaml
recipe: default.v1
language: en
pipeline:
  - name: WhitespaceTokenizer
  - name: RegexFeaturizer
  - name: LexicalSyntacticFeaturizer
  - name: CountVectorsFeaturizer
  - name: DIETClassifier
    epochs: 50
  - name: FallbackClassifier
    threshold: 0.35
    ambiguity_threshold: 0.1

policies:
  - name: RulePolicy
    core_fallback_action_name: "utter_out_of_scope"
    core_fallback_threshold: 0.3
```

Add minimal NLU examples (create/extend `data/nlu.yml`):

```yaml
version: "3.1"
nlu:
  - intent: ask_kb
    examples: |
      - What is AiHub?
      - Tell me about aihub.org.za services
      - How do I contact AiHub?
      - Where is AiHub based?
      - Do you have pricing?
      - What do you offer?

  - intent: out_of_scope
    examples: |
      - Tell me about the weather
      - Translate this to French
      - Who is the president of South Africa?
```

Implement the retriever (replace your `actions/actions.py` with the below or merge it):

```python
# actions/actions.py
import os, json
from typing import Any, Text, Dict, List, Tuple
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer

from rasa_sdk import Action, Tracker
from rasa_sdk.executor import CollectingDispatcher
from rasa_sdk.events import EventType

IDX_DIR = "kb/index"
SIM_THRESHOLD = 0.52   # tune: raise to be stricter
TOP_K = 4

class KB:
    _loaded = False
    index = None
    meta = None
    model = None

    @classmethod
    def load(cls):
        if cls._loaded: 
            return
        cls.index = faiss.read_index(os.path.join(IDX_DIR, "faiss.index"))
        with open(os.path.join(IDX_DIR, "meta.json"), "r", encoding="utf-8") as f:
            cls.meta = json.load(f)
        cls.model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
        cls._loaded = True

    @classmethod
    def search(cls, query: str, top_k: int = TOP_K) -> List[Tuple[float, Dict]]:
        cls.load()
        q = cls.model.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype("float32")
        sims, idxs = cls.index.search(q, top_k)
        results = []
        for s, i in zip(sims[0].tolist(), idxs[0].tolist()):
            if i == -1:
                continue
            results.append((float(s), cls.meta[i]))
        return results

def format_answer(hits: List[Tuple[float, Dict]]) -> str:
    # stitch top chunks; add tiny citations (urls)
    parts = []
    used = set()
    for score, doc in hits:
        if score < SIM_THRESHOLD:
            continue
        url = doc["url"]
        if url in used: 
            continue
        used.add(url)
        snippet = doc["text"].strip()
        # cap snippet length to avoid rambling
        snippet = (snippet[:600] + "â€¦") if len(snippet) > 600 else snippet
        parts.append(f"{snippet}\n\nSource: {url}")
        if len(parts) >= 2:  # at most 2 sources in one reply
            break
    return "\n\n---\n\n".join(parts)

class ActionRetrieveAnswer(Action):
    def name(self) -> Text:
        return "action_retrieve_answer"

    def run(
        self, dispatcher: CollectingDispatcher, tracker: Tracker, domain: Dict[Text, Any]
    ) -> List[EventType]:
        user_q = tracker.latest_message.get("text", "").strip()
        if not user_q:
            dispatcher.utter_message(text="Please ask a question about aihub.org.za.")
            return []

        hits = KB.search(user_q, top_k=TOP_K)
        answer = format_answer(hits)

        if not answer:
            dispatcher.utter_message(text="I only answer using information from aihub.org.za. I couldnâ€™t find anything relevant for your question.")
            return []

        # Guardrail: we explicitly state the constraint
        dispatcher.utter_message(text=answer)
        return []
```

> This approach **never** hallucinates: it replies only with stitched snippets from the crawled site and refuses low-similarity queries.

Start services:

```bash
./dev.sh actions
./dev.sh shell
```

---

## 4) Keep responses strictly in-domain (hard guardrails)

* Similarity gate: `SIM_THRESHOLD` ensures we only answer when a close match exists.
* Fallbacks: unknown/low-confidence â†’ `utter_out_of_scope`.
* Intent gating: guide users into `ask_kb` with your NLU data; expand examples over time.

> If you want an even harder lock: inside `ActionRetrieveAnswer`, if **any** top score `< SIM_THRESHOLD`, just refuse entirely (no partial snippets).

---

## 5) (Optional) Add quick FAQ intents

For very common questions, you can still use Rasaâ€™s `ResponseSelector` or plain `responses:` so the bot answers instantly without retrieval. Retrieval remains the fallback for long-tail questions.

---

## 6) Re-index when the site changes

Anytime the website is updated:

```bash
python tools/crawl_site.py
python tools/index_kb.py
```

No Rasa retraining required for content updatesâ€”only for NLU tweaks.

---

## 7) Where to go next

* **Citations formatting**: turn the â€œSource:â€ lines into clickable markdown if youâ€™re piping messages to a UI.
* **Re-ranking**: add a lightweight re-rank (e.g., `sklearn` cosine on smaller chunks) if you want even tighter answers.
* **Channel**: connect Rasa to your chosen channel (Chatwoot/WhatsApp) once happy with grounding.

---

If you want, I can also add:

* a `Makefile` task (`make crawl`, `make index`) or
* menu entries in your `dev.sh` (`kb-crawl`, `kb-index`)
  Just say the word and Iâ€™ll drop those in.
